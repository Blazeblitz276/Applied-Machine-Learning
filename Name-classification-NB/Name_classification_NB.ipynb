{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Name Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Dataset\n",
    "\n",
    "Adding column names beacuse data is missing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirpath = '../data'\n",
    "columns = ['names', 'label']\n",
    "us = pd.read_csv(os.path.join(dirpath,'us.txt'),names = columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Moses</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna Barajas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James Caldwell</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. Michael Cole</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jeffrey Collier</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              names  label\n",
       "0     Timothy Moses    NaN\n",
       "1      Anna Barajas    NaN\n",
       "2    James Caldwell    NaN\n",
       "3  Mr. Michael Cole    NaN\n",
       "4   Jeffrey Collier    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "us.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the same with every language file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "japan = pd.read_csv(os.path.join(dirpath,'japan.txt'),names = columns)\n",
    "greek = pd.read_csv(os.path.join(dirpath,'greek.txt'),names = columns)\n",
    "arabic = pd.read_csv(os.path.join(dirpath,'arabic.txt'),names = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding labels manually to every language dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['us']*len(us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Moses</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna Barajas</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James Caldwell</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. Michael Cole</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jeffrey Collier</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              names label\n",
       "0     Timothy Moses    us\n",
       "1      Anna Barajas    us\n",
       "2    James Caldwell    us\n",
       "3  Mr. Michael Cole    us\n",
       "4   Jeffrey Collier    us"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = ['japan']*len(japan)\n",
    "label2 = ['greek']*len(greek)\n",
    "label3 = ['arabic']*len(arabic)\n",
    "japan['label'] = label1\n",
    "greek['label'] = label2\n",
    "arabic['label'] = label3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all 4 languages to create a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalnames = pd.concat([us,japan,greek,arabic],axis = 0,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for null or irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names    0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finalnames.shape\n",
    "finalnames.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting dataset into 70 % train and 30 % test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train , test = train_test_split(finalnames,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Naive Bayes classifier\n",
    "\n",
    "Preprocessing function to split names into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess(line):\n",
    "    words = line.strip().split(\" \")\n",
    "    #print(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculating Naive Bayes probability and predicting on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the parameter estimation formula in the given resources in order to implement Naive Bayes classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def result(sentence,vals):\n",
    "\n",
    "    us_words,arabic_words,japan_words,greek_words = vals[0],vals[1],vals[2],vals[3]\n",
    "    us_names,arabic_names,japan_names,greek_names = vals[4],vals[5],vals[6],vals[7]\n",
    "    total_us_words,total_arabic_words,total_japan_words,total_greek_words = vals[8],vals[9],vals[10],vals[11]\n",
    "    list_words = preprocess(sentence)\n",
    "    p_us,p_arabic,p_japan,p_greek = 0,0,0,0\n",
    "    total_us_words += len(us_words)*0.3\n",
    "    total_arabic_words += len(arabic_words)* 0.3\n",
    "    total_japan_words += len(japan_words)* 0.3\n",
    "    total_greek_words += len(greek_words)* 0.3\n",
    "\n",
    "    for word in list_words:\n",
    "        fus = 0\n",
    "        if word in us_words.keys():fus = us_words[word]\n",
    "        farab = 0\n",
    "        if word in arabic_words.keys():farab = arabic_words[word]\n",
    "        fjap = 0\n",
    "        if word in japan_words.keys():fjap = japan_words[word]\n",
    "        fgreek = 0\n",
    "        if word in greek_words.keys():fgreek = greek_words[word]\n",
    "\n",
    "        p_us += math.log2((0.3 + fus)/total_us_words)\n",
    "        p_arabic += math.log2((0.3 + farab)/total_arabic_words)\n",
    "        p_japan += math.log2((0.3 + fjap)/total_japan_words)\n",
    "        p_greek += math.log2((0.3 + fgreek)/total_greek_words)\n",
    "    \n",
    "    total = us_names + arabic_names + japan_names + greek_names\n",
    "    usa = us_names/(total) \n",
    "    arabica = arabic_names/(total) \n",
    "    japana = japan_names/(total) \n",
    "    greeka = greek_names/(total)\n",
    "    \n",
    "    p_us += math.log2(usa)\n",
    "    p_arabic += math.log2(arabica)\n",
    "    p_japan += math.log2(japana)\n",
    "    p_greek += math.log2(greeka)\n",
    "\n",
    "    maxx = max(p_us,p_arabic, p_japan,p_greek)\n",
    "\n",
    "    if maxx == p_us:return 'us'\n",
    "    elif maxx == p_arabic:return 'arabic'\n",
    "    elif maxx == p_japan:return 'japan'\n",
    "    elif maxx == p_greek:return 'greek'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to preprocess dataset and generate vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used dictionaries in order to create a vocabulary for the names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(train_data, test_data):\n",
    "\n",
    "    us_words,arabic_words,greek_words,japan_words = {},{},{},{}\n",
    "    us_names,arabic_names,japan_names,greek_names = 0,0,0,0\n",
    "    total_us_words,total_arabic_words,total_japan_words,total_greek_words = 0,0,0,0\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        processed_words = preprocess(train_data['names'].iloc[i])\n",
    "        #print(processed_words)\n",
    "        if train_data['label'].iloc[i] == 'us':\n",
    "            us_names += 1\n",
    "            total_us_words += len(processed_words)\n",
    "            for j in processed_words:\n",
    "                if j not in us_words:us_words[j] = 1\n",
    "                else:us_words[j] += 1\n",
    "\n",
    "        elif train_data['label'].iloc[i] == 'arabic':\n",
    "            arabic_names += 1\n",
    "            total_arabic_words += len(processed_words)\n",
    "            for j in processed_words:\n",
    "                if j not in arabic_words:arabic_words[j] = 1\n",
    "                else:arabic_words[j] += 1\n",
    "\n",
    "        elif train_data['label'].iloc[i] == 'greek':\n",
    "            greek_names += 1\n",
    "            total_greek_words += len(processed_words)\n",
    "            for j in processed_words:\n",
    "                if j not in greek_words:greek_words[j] = 1\n",
    "                else:greek_words[j] += 1\n",
    "\n",
    "        else:\n",
    "            japan_names += 1\n",
    "            total_japan_words += len(processed_words)\n",
    "            for j in processed_words:\n",
    "                if j not in japan_words:japan_words[j] = 1\n",
    "                else:japan_words[j] += 1\n",
    "\n",
    "    \n",
    "    vals = []\n",
    "    vals.append(us_words)\n",
    "    vals.append(arabic_words)\n",
    "    vals.append(japan_words)\n",
    "    vals.append(greek_words)\n",
    "\n",
    "    vals.append(us_names)\n",
    "    vals.append(arabic_names)    \n",
    "    vals.append(japan_names)\n",
    "    vals.append(greek_names)\n",
    "\n",
    "    vals.append(total_us_words)\n",
    "    vals.append(total_arabic_words)\n",
    "    vals.append(total_japan_words)\n",
    "    vals.append(total_greek_words)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        ans = result(test_data['names'].iloc[i],vals)\n",
    "        res.append(ans)\n",
    "\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy = 92.50%\n"
     ]
    }
   ],
   "source": [
    "results= classifier(train, test)\n",
    "\n",
    "correct_ct = sum([ (results[i] == test['label'].iloc[i]) for i in range(0, len(test)) ])\n",
    "print(\"Classification accuracy = %5.2f%%\" % (100.0 * correct_ct / len(test)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
